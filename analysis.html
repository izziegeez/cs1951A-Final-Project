<html>
<head>
    <link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Data</title>
</head>
<style>
li{
	font-size: 15px;
}
</style>

<body>

<div class="row">
    <div class="col-md-2"></div>
    <div class="col-md-8">
        <div class="page-header center">
            <h1>Bogus Of The Catastrophic<br>
                <<< Analysis Deliverable>>></h1>
        </div>

         <h1> Github Repository</h1>
        	<ul>
            <li> <a href="https://github.com/izziegeez/cs1951A-Final-Project"> Github Link </a> </li>
        	</ul>

       	<h1> Questions that Concern Us</h1>
       		<ul>
       			<li> In face of a global pandemic that very few people have any prior knowledge and experience about, media coverage is critical to inform the public about the situation. With such substantial reliance on the mainstream media, its capability of reflecting the pandemic situation and accuracy of portraying the seriousness of the issue is critical to influence the behavior of the general public, which is in turn crucial to the efforts of combating the pandemic. Therefore, we are interested in analyzing the media reports on the COVID-19 pandemic and determine if they are adequate in conveying the severity of the outbreak.</li>

       		</ul>


        <h1> Prediction </h1>
        	<ul>
            <li> At its core, our project focuses on prediction tasks. We have two sets of data, that of the pandemic itself, which includes measures such as number of cases, growth of cases, number of deaths, and so forth, and that of the media reports, including their titles, keywords, articles, and so forth. </li>
            <ul>
		        <li>  To generalize our task,<b> we want to ask whether we can predict the severity of the pandemic by analyzing the media coverage from that day. </b></li>
		        <li> To be more specific, we count keyword frequency in everyday news articles to represent media coverage and use numbers of deaths and confirmed cases as indicators of pandemic severity. We want to eventually use frequencies of selected keywords of a given day to predict the severity level of pandemic of that day. In other words, we want to have a general idea of the severity level of disease by just simply looking at news coverage.
				 </li>
			</ul>
			<li> To elaborate on its metrics, the severity of the pandemic is captured by the growth rate of new cases and deaths, and we analyze media reports based on the words that appear therein, both their presence and the frequencies at which each of them appears. If it is indeed possible to predict the severity of the pandemic by analyzing the news reports, it would lend evidence towards the reliability of news media being able to relatively accurately report on the scale of the issue. If not, it does not necessarily mean the media reports are not reflective of the actual situation, but it at least is not noticeably predictive of the pandemic.
			</li>

        	</ul>

        <h1> Data Spec</h1>
        	<ul> 
        		<li>
        			Overview
        		<ul>
        			<li>
        				We have two main data sources for raw data:
        				<ul>
        					<li> A news database generated by an open source API <a href="https://contextualweb.io/news-api/"> News API link </a></li>
        					<li>Confirmed/Deaths numbers world-wide by JHU <a href="https://coronavirus.jhu.edu/map.html"> JHU Coronavirus Data Link </a></li>
        				</ul>
        			</li>
        			<li>
        				<b>Technical tools </b>: SQL/Python/Sqlite3/Numpy <a href="https://github.com/izziegeez/cs1951A-Final-Project/tree/master/data"> Link to Data Folder </a>
        			</li>
        		</ul>
        		</li>




        	</ul>

        	<ul>
        			<li>Raw Data
        				<ul>
        					<li>News raw data
        						<ul>
        							<li>We gathered news articles from a third-party news APIs -Contextual Web API, which includes most mainstream new outlets. Using search terms ‘coronavirus’, ‘COVID’, ‘nCov’, and other search terms that are closely related to the current epidemic, we gathered news articles in English from across the world that report on the issue from January 7th, 2020 to April 11th 2020. The resulted data includes article titles, date, body, provider, and keywords.</li>
        							<ul>
        								<li>Data Specs: Table 'articles'</li>
        								<ul>
        								<li> date: @the date the news article was published</li>
        								<li> title: @the title of the article</li>
        								<li> url: @the url of the article</li>
        								<li> description: @a summary of the content of the article</li>
        								<li> body: @an excerpt from the article</li>
        								<li> keywords: @the central themes of the article. Words and phrases separated by comma</li>
        								<li> language: @the language in which the article is written</li>
        								<li> provider: @the news outlet that published the article</li>
        								</ul>
        							</ul>
        							 </ul>
        					</li>
        						<li>Confirmed cases/deaths worldwide per day(time_series_covid19_confirmed_global.csv/time_series_covid19_death_global.csv)
        							<ul>
        								<li>
        									We also gathered worldwide coronavirus confirmed cases and deaths data from JHU Coronavirus Resource Center, which provides data from January 22nd to April 12th. To gather data prior to January 22nd, when the coronavirus outbreak mainly only occurred in Wuhan, China, we visited Wuhan’s Municipal Health Commission’s official website, where the Health Commission announced coronavirus cases and deaths in Wuhan City starting from December 31st, 2019. 
        								</li>
        							</ul>
        						</li>
        					
        				</ul>

        			</li>
        		</ul>

    		<ul>
    			<li>Processed Data
    				<ul>
    					<li>After gathering data, we calculated our independent and dependent variables(“severity” and “keywords”) and stored all data into news.db(training) and news_testing.db(testing). We used Python and SQL scripts to combine two data sources, join tables, and clean data.
    						<ul>
    							<li>
    								Training: news.db (January 7th, 2020 - March 8th, 2020)
    								<ul>
    									<li>Dependent variable(severity/growth rate)
    										<ul>
    											<li>We further cleans our News raw data to include a “severity” table, which is calculated by the growth rate of new cases and deaths from the previous day(represented by a number from 0-1). </li>
    											<li> Table ‘severity’ schema:
    												<ul> 
    													<li>Date: @date</li>
    													<li>Deaths_no: @number of deaths in the given date</li>
    													<li>Confirmed_no: @number of confirmed cases in the given date</li>
    												</ul>
    											</li>
    											
    										</ul>

    									</li>

    									<li>Independent variable (keywords frequencies)
    										<ul> 
    											<li> We added a “keyword” table as well, which counts the frequency of each word that appeared in all articles of that day, and the number of articles that keyword has occurred in. </li>
    											<li> We pre-filtered out words that have character lengths fewer than 5, which are mainly propositions. We also came up with a set of irrelevant words such as “about”, “after”, “could”, etc. that we did not include in the keyword set. The resulted keywords table ordered by frequency are hugely relevant to the coronavirus outbreak such as “health”, “lockdown”, “pandemic”, “cases”, “hospital”, “government”, etc.  </li>
    											<li> Table ‘keywords’ schema:
    												<ul>
    													<li>Date: @date</li>
    													<li>Keyword: @keyword parsed from one body paragraph of each articles</li>
    													<li>Frequency: @frequency of each keyword of the given date</li>
    													<li>Num_articles: @total number of articles for the given date</li>
    												</ul>
    											 </li>
    										</ul>
    									</li>
    								</ul>
    							</li>
    							<li>
    								Testing: news_testing.db (March 28th, 2020 - April 11th, 2020)
    								<ul>
    									<li>
    										We generated a severity table and a keywords table using the same method as outlined above for news from March 28th, 2020 to April 11th, 2020, which was used to test our training model.
    									</li>
    								</ul>
    							</li>
    						</ul>
    					</li>
    				</ul>
    			</li>
    		</ul>

    		<h3> Why did you use this statistical test or ML algorithm? Which other tests did you consider or evaluate? How did you measure success or failure? Why that metric/value? What challenges did you face evaluating the model? Did you have to clean or restructure your data?</h3>
    		<ul>
    			<li>Overview:
    				<ul>
    					<li>We used three ML algorithms--Naive Bayes, Logistic Regression, and Neural Network--to develop models that can predict real-time pandemic conditions by looking at news coverage.</li>
    					<li>Technical tools: Pytorch/Matplot/SQL/Python/Sqlite3/Numpy</li>
    					<li>Inputs: 2D numpy array</li>
    					 	<ul>
    					 		<li>Size: num of dates * num of selected keywords</li>
    					 		<li>Row: corresponding to each date</li>
    					 		<li>Column: keyword frequency</li>
    					 	</ul>
    					<li>Outputs: 2D numpy array
    						<ul>
    					 		<li>Size: num of dates * 2</li>
    					 		<li>Row: corresponding to each date</li>
    					 		<li>Column: growth rates of deaths number and confirmed cases number</li>
    					 	</ul>
    					</li>
    				</ul>
    			</li>
    			<li> Naive Bayes: <a href="https://github.com/izziegeez/cs1951A-Final-Project/tree/master/Naive_Bayes"> Github Link </a> 
    				<ul> 
    					<li> One of the models we used is Naive Bayes. The broad idea for using this model is to test if the words that are used in media reports each time can be used to predict whether the pandemic development that day is severe. </li>
    					<li> Therefore, for the input, we use each day as a data sample, and record what words were used in the report that day as the features of the sample. For the labels, we decided to set a threshold on the growth rate of new confirmed cases, labeling a day that has more than 10% growth in confirmed cases than the previous day as 1, and those less than 10% as 0. </li>
    					<li> We trained the model on two months (62 days) worth of data, and validated the model using two weeks (15 days) worth of testing data. Testing the model is simply done by feeding the model the validation (testing) data set, and checking the accuracy of the predictions. The accuracy of the model on predicting the testing data is used as a measure of success. </li>
    					<li>We made several decisions in regards to the data.
    						 <ul>
    						 	<li> First, the amount of news report data is large, and if we were to use all words that appeared in them, it would lead to a tremendous amount of features that are not all useful to the task. Therefore, we first eliminated all the words that only serve grammatical purposes but have no substantial meaning. </li>
    						 	<li> Among those words, we then also chose only words that appear frequently enough (being used more than 50 times among the training data). Given these restructuring of the data, the Naive Bayes model performed satisfactorily.  </li>
    						 </ul>
    					</li>
    				</ul>
    			</li>
    			<li>Logistic Regression: <a href="https://github.com/izziegeez/cs1951A-Final-Project/tree/master/Logistic_Regression"> Github Link </a> 
    				<ul>
    					<li>We also used logistic regression to assess whether there exists a linear correlation between news coverage and case numbers/real-time pandemic situations. If there existed one, then we could use a linear function to predict future pandemic situations based on news coverage.</li>
    					<li> Since logistic regression is a supervised learning algorithm and requires labels to be either 0 or 1, we set labels with a growth rate larger than 15% to be 1, otherwise 0.</li>
    					<li> We used sigmoid function to update our parameters and looked at loss values to assess whether we could use logistic regression to develop a good model to depict the relationship between news coverage and real-time pandemic situations. We also used testing data to assess the model.</li>
    				</ul>
    			 </li>

    			 <li> Neural Network: <a href="https://github.com/izziegeez/cs1951A-Final-Project/tree/master/Neural_Network"> Github Link </a> 
    			 </li>
    		</ul>

    		<h3>What is your interpretation of the results? Do accept or deny the hypothesis, or are you satisfied with your prediction accuracy? For prediction projects, we expect you to argue why you got the accuracy/success metric you have. Intuitively, how do you react to the results? Are you confident in the results? </h3>
    		<ul> 
    			<li>Naive Bayes: 
    				<ul>
    					<li>The results from Naive Bayes are reasonably satisfactory, achieving a validation accuracy of 0.8. More concretely, from 3/28 to 4/11, we labeled days where the number of confirmed cases increased by more than 10% globally as 1, and those lower than 10% as 0. The trained Naive Bayes model (trained from data from 1/7 to 3/8) predicted 80% of the classifications correctly. Admittedly, the validation data set is relatively small, consisting only 15 samples total, and the threshold of 10% increase is chosen based on perceived seriousness. These limitations undermine the informativeness of the results, and we are not entirely confident about this result, since the model utilized is relatively simple. However, it is still relatively clear that there exists some form of variation in the usage of words in media reports that reflect the seriousness of pandemic development of the day.</li>
    					</ul>
    			</li>
    			<li>Logistic Regression:
    				<ul>
    					<li> The results from logistic regression are less satisfactory than naive bayes. Although the loss function shows an exponential decrease in loss as the epoch number increases, due to the limited number of training data (62 days), the model may be overfitting. (insert visualization/Linear_Regression_Plot.png)</li>
    					<li> As a result, the accuracy results of this logistic regression model is relatively large compared with that of the naive bayes. (insert another png, ask me when you are here!)</li>
    					<li>Besides overfitting, another reason for this not so ideal model may be that by categorizing actual growth rates into 0 and 1, we lost much information on labels and this fact may negatively contribute to the results. </li>
    				</ul>

    			</li>
    			<li>Neural Network: 
    			</li>
    		</ul>

    		<h3> For your visualization, why did you pick this graph? What alternative ways might you communicate the result? Were there any challenges visualizing the results, if so, then what were they? Will your visualization require text to provide context or is it standalone (either is fine, but it's recognize which type your visualization is)? </h3>


    		<h3>Full results + graphs (at least 1 stats/ml test and at least 1 visualization). Depending on your model/test/project we would ideally like you to show us your full process so we can evaluate how you conducted the test! </h3>


    		<h3>If you did a machine learning model, why did you choose this machine learning technique? Does your data have any sensitive/protected attributes that could affect your machine learning model? </h3>
    		<ul>
    			<li>Naive Bayes:  
    				<ul>
    					<li>This model is chosen, because the nature of the textual data from media reports lends itself to processing them as a ‘bag of words’, and Naive Bayes is a relatively straightforward model to classify textual inputs. The main concern in terms of the fairness of the model is the source of the data, the news media outlets/providers to be more specific. The API used draws from only a subset of all news media, and thus the occluded sources may instill biases into the analysis, if the chosen media outlets are more or less reflective of the pandemic than the ones that are not chosen. The uniformity of the language of the news articles, namely they are all in English, is also a potential cause of bias, as these sources might likely focus more on English-speaking regions and react towards pandemic outbreaks in those regions more strongly than the rest of the world. Therefore, these traits of the data sample might be of concern. </li>
    					</ul>
    			</li>
    			<li>Logistic Regression:
    				<ul>
    					<li> We chose logistic regression because it is widely regarded as an effective categorization algorithm. However, due to the relatively small number of training data points (62 days) and a generalization of labels data, the model might be overfitting.  </li>
    				 </ul>
    			</li>
    			<li>Neural Network:  </li>
    		</ul>

    		<h1> Future Directions: </h1>
    		<ul>
    			<li> One of the biggest challenges that we are facing in this project is the lack of data points. Since COVID-19 became popular on news only since the end of January, we only have a very limited number of dates. As a result, our models have a relatively large prediction loss due to overfitting. We hope that we will continue keeping track of daily news coverage and gather more data points.</li>
    			<li> Due to the lack of experience in using ML algorithms, we spent a large amount of time trying to decide which algorithms to use. We started off with linear regression, and then gradually expanded our efforts to logistic regression, naive bayes, and then neural networks. We hope that in the future we can be capable of exploring more algorithms.</li>
    			<li> The extent of our analysis is also relatively limited due to the scale of our data. We had trouble gathering data from other languages, as cleaning and translation are more difficult than anticipated. Given more time and resources, it would be very interesting to compare trends across different languages and see how media reports from different regions react.</li>
    			<li>The models that we decided to employ are also somewhat limited by the data. Given more extensive data, we can potentially conduct sentiment analysis to better capture the media reactions. Along the same tangent, we could also conduct more NLP on the data and better understand what topics are popular among the new outlets. We also attempted a neural network, which worked to some extent, but having a larger quantity of data can definitely help train the model much better. </li>


    		</ul>




    </div>
</div>

</body>
</html>
